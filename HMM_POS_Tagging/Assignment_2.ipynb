{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6fbf86-2726-48de-903c-d0e71c6b752c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: C:\\Users\\haric\\hmm_pos_env\\Scripts\\python.exe\n",
      "Python version: 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]\n",
      "NumPy version: 2.3.5\n",
      "All set! You're in a clean environment. No NLTK/spaCy/sklearn allowed — we're good!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"All set! You're in a clean environment. No NLTK/spaCy/sklearn allowed — we're good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18bd5d50-0f9c-4058-9070-f136af4de7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists: en_ewt-ud-train.conllu (14.3 MB)\n",
      "You are ready to run the full HMM POS Tagger code!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "dataset_file = \"en_ewt-ud-train.conllu\"\n",
    "\n",
    "if not os.path.exists(dataset_file):\n",
    "    print(\"Downloading Universal Dependencies English-EWT training data (~35 MB)...\")\n",
    "    url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\"\n",
    "    urlretrieve(url, dataset_file)\n",
    "    print(f\"Download complete! Saved as {dataset_file}\")\n",
    "    print(f\"File size: {os.path.getsize(dataset_file) / (1024*1024):.1f} MB\")\n",
    "else:\n",
    "    print(f\"Dataset already exists: {dataset_file} ({os.path.getsize(dataset_file) / (1024*1024):.1f} MB)\")\n",
    "\n",
    "print(\"You are ready to run the full HMM POS Tagger code!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c7d15b-67ac-4913-a652-885bd259f563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 12544, Train: 10035, Test: 2509\n",
      "Unique tags: 17, Unique words: 17515\n",
      "\n",
      "Top 10 Transition Probabilities:\n",
      "P(VERB | PART) = 0.6935\n",
      "P(NOUN | DET) = 0.5925\n",
      "P(NOUN | ADJ) = 0.5216\n",
      "P(X | X) = 0.5185\n",
      "P(NUM | SYM) = 0.5122\n",
      "P(PRON | SCONJ) = 0.4782\n",
      "P(PUNCT | INTJ) = 0.3724\n",
      "P(DET | ADP) = 0.3614\n",
      "P(NOUN | NUM) = 0.3492\n",
      "P(VERB | AUX) = 0.3372\n",
      "\n",
      "Top 10 Emission Probabilities:\n",
      "P(June | PROPN) = 0.0010\n",
      "P(handle | VERB) = 0.0003\n",
      "P(confirm | VERB) = 0.0003\n",
      "P(answer | VERB) = 0.0003\n",
      "P(Martin | PROPN) = 0.0003\n",
      "P(arrived | VERB) = 0.0003\n",
      "P(settled | VERB) = 0.0003\n",
      "P(Bangs | PROPN) = 0.0002\n",
      "P(White | PROPN) = 0.0001\n",
      "P(X940 | PROPN) = 0.0001\n",
      "\n",
      "Accuracy on test data: 0.8628 (736/853)\n",
      "\n",
      "Example Outputs for 3 Sample Sentences:\n",
      "\n",
      "Sample 1:\n",
      "Sentence: Lorie Leigh @ ECT\n",
      "Gold Tags: PROPN X X X\n",
      "Predicted Tags: PROPN X X X\n",
      "\n",
      "Sample 2:\n",
      "Sentence: They need to update the locker rooms ASAP .\n",
      "Gold Tags: PRON VERB PART VERB DET NOUN NOUN ADV PUNCT\n",
      "Predicted Tags: PRON VERB PART VERB DET ADJ NOUN ADV PUNCT\n",
      "\n",
      "Sample 3:\n",
      "Sentence: There is a lot to learn about Chernobyl .\n",
      "Gold Tags: PRON VERB DET NOUN PART VERB ADP PROPN PUNCT\n",
      "Predicted Tags: PRON AUX DET NOUN PART VERB ADP PROPN PUNCT\n",
      "\n",
      "Error Analysis Example:\n",
      "Sentence: They need to update the locker rooms ASAP .\n",
      "Gold Tags: PRON VERB PART VERB DET NOUN NOUN ADV PUNCT\n",
      "Predicted Tags: PRON VERB PART VERB DET ADJ NOUN ADV PUNCT\n",
      "Analysis:\n",
      " - Word 'locker' mis-tagged as ADJ (gold: NOUN)\n",
      "   Possible Reason: Word ambiguity or contextual influence from transitions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Step 1: Download and parse the dataset\n",
    "# Function to parse CoNLL-U format into list of sentences, each a list of (word, tag) pairs.\n",
    "def parse_conllu(filename):\n",
    "    data = []\n",
    "    sent = []\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    if sent:\n",
    "                        data.append(sent)\n",
    "                        sent = []\n",
    "                elif line.startswith('#'):\n",
    "                    continue\n",
    "                else:\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) >= 4 and '-' not in parts[0]:  # Skip multi-word tokens and empty fields\n",
    "                        word = parts[1] if len(parts) > 1 else ''\n",
    "                        tag = parts[3] if len(parts) > 3 else ''\n",
    "                        if word and tag:  # Only add if both are non-empty\n",
    "                            sent.append((word, tag))\n",
    "        if sent:\n",
    "            data.append(sent)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {filename} not found. Please download it manually.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Download the dataset if not exists\n",
    "dataset_file = 'en_ewt-ud-train.conllu'\n",
    "if not os.path.exists(dataset_file):\n",
    "    try:\n",
    "        url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu'\n",
    "        urlretrieve(url, dataset_file)\n",
    "        print(f\"Downloaded dataset to {dataset_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed ({e}). Please download manually:\")\n",
    "        print(\"1. Go to: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\")\n",
    "        print(\"2. Right-click → Save As → en_ewt-ud-train.conllu in your notebook folder.\")\n",
    "        print(\"3. Re-run this cell.\")\n",
    "\n",
    "# Load and shuffle the data\n",
    "data = parse_conllu(dataset_file)\n",
    "if not data:\n",
    "    print(\"No data loaded. Exiting.\")\n",
    "else:\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(data)\n",
    "    # Split into 80% train and 20% test\n",
    "    train_size = int(0.8 * len(data))\n",
    "    train = data[:train_size]\n",
    "    test = data[train_size:]\n",
    "    print(f\"Total sentences: {len(data)}, Train: {len(train)}, Test: {len(test)}\")\n",
    "\n",
    "    # Step 2: Training Phase\n",
    "    # Compute counts for transitions, emissions, and tags.\n",
    "    tag_counts = defaultdict(int)\n",
    "    trans_counts = defaultdict(lambda: defaultdict(int))\n",
    "    emis_counts = defaultdict(lambda: defaultdict(int))\n",
    "    start = '<START>'\n",
    "\n",
    "    # Count starts, transitions, emissions, and tag occurrences\n",
    "    tag_counts[start] = len(train)\n",
    "    for sent in train:\n",
    "        if not sent:\n",
    "            continue\n",
    "        prev = start\n",
    "        for word, tag in sent:\n",
    "            trans_counts[prev][tag] += 1\n",
    "            emis_counts[tag][word] += 1\n",
    "            tag_counts[tag] += 1\n",
    "            prev = tag\n",
    "\n",
    "    # Get unique tags (excluding <START>) and words\n",
    "    unique_tags = list(set(tag for sent in train for _, tag in sent))\n",
    "    unique_words = set(word for sent in train for word, _ in sent)\n",
    "    V_trans = len(unique_tags)  # Vocabulary size for transitions (number of tags)\n",
    "    V_emis = len(unique_words)  # Vocabulary size for emissions\n",
    "\n",
    "    print(f\"Unique tags: {len(unique_tags)}, Unique words: {len(unique_words)}\")\n",
    "\n",
    "    # Step 3: Display top 10 transition and emission probabilities\n",
    "    # Collect and sort transition probabilities\n",
    "    trans_probs = []\n",
    "    for prev in [start] + unique_tags:\n",
    "        for tag in unique_tags:\n",
    "            count = trans_counts[prev][tag]\n",
    "            prob = (count + 1) / (tag_counts[prev] + V_trans)\n",
    "            trans_probs.append((prev, tag, prob))\n",
    "    trans_probs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(\"\\nTop 10 Transition Probabilities:\")\n",
    "    for i in range(min(10, len(trans_probs))):\n",
    "        prev, tag, prob = trans_probs[i]\n",
    "        print(f\"P({tag} | {prev}) = {prob:.4f}\")\n",
    "\n",
    "    # Collect and sort emission probabilities (limit to top for large vocab to avoid long computation)\n",
    "    emis_probs = []\n",
    "    for tag in unique_tags[:5]:  # Limit to top 5 tags for speed; adjust if needed\n",
    "        tag_count = tag_counts[tag]\n",
    "        for word in list(unique_words)[:100]:  # Sample 100 words to keep it fast\n",
    "            count = emis_counts[tag][word]\n",
    "            prob = (count + 1) / (tag_count + V_emis)\n",
    "            emis_probs.append((tag, word, prob))\n",
    "    emis_probs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(\"\\nTop 10 Emission Probabilities:\")\n",
    "    for i in range(min(10, len(emis_probs))):\n",
    "        tag, word, prob = emis_probs[i]\n",
    "        print(f\"P({word} | {tag}) = {prob:.4f}\")\n",
    "\n",
    "    # Step 4: Decoding Phase - Viterbi Algorithm\n",
    "    # Function to predict tags for a sentence using Viterbi with log probs.\n",
    "    def viterbi(words):\n",
    "        if not words:\n",
    "            return []\n",
    "        states = unique_tags\n",
    "        n = len(words)\n",
    "        v = [{} for _ in range(n)]  # Viterbi probs\n",
    "        bp = [{} for _ in range(n)]  # Backpointers\n",
    "\n",
    "        # Initialization\n",
    "        for state in states:\n",
    "            trans_prob = (trans_counts[start][state] + 1) / (tag_counts[start] + V_trans)\n",
    "            emis_prob = (emis_counts[state].get(words[0], 0) + 1) / (tag_counts[state] + V_emis)\n",
    "            v[0][state] = math.log(trans_prob) + math.log(emis_prob)\n",
    "            bp[0][state] = start\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(1, n):\n",
    "            for state in states:\n",
    "                max_prob = float('-inf')\n",
    "                max_prev = None\n",
    "                for prev in states:\n",
    "                    trans_prob = (trans_counts[prev][state] + 1) / (tag_counts[prev] + V_trans)\n",
    "                    emis_prob = (emis_counts[state].get(words[t], 0) + 1) / (tag_counts[state] + V_emis)\n",
    "                    prob = v[t-1][prev] + math.log(trans_prob) + math.log(emis_prob)\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        max_prev = prev\n",
    "                v[t][state] = max_prob\n",
    "                bp[t][state] = max_prev\n",
    "\n",
    "        # Backtrack\n",
    "        path = [None] * n\n",
    "        path[n-1] = max(states, key=lambda s: v[n-1][s])\n",
    "        for t in range(n-1, 0, -1):\n",
    "            path[t-1] = bp[t][path[t]]\n",
    "        return path\n",
    "\n",
    "    # Step 5: Evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errors = []  # To collect sentences with errors for analysis\n",
    "    for sent in test[:50]:  # Limit to first 50 for speed; remove limit for full eval\n",
    "        words = [w for w, t in sent]\n",
    "        gold_tags = [t for w, t in sent]\n",
    "        pred_tags = viterbi(words)\n",
    "        if len(pred_tags) != len(gold_tags):\n",
    "            continue  # Skip if mismatch\n",
    "        for p, g in zip(pred_tags, gold_tags):\n",
    "            if p == g:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        if pred_tags != gold_tags:\n",
    "            errors.append((words, gold_tags, pred_tags))\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\nAccuracy on test data: {accuracy:.4f} ({correct}/{total})\")\n",
    "\n",
    "    # Step 6: Example output for 3 sample sentences\n",
    "    print(\"\\nExample Outputs for 3 Sample Sentences:\")\n",
    "    for i in range(min(3, len(test))):\n",
    "        sent = test[i]\n",
    "        words = [w for w, t in sent]\n",
    "        gold_tags = [t for w, t in sent]\n",
    "        pred_tags = viterbi(words)\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(\"Sentence:\", ' '.join(words))\n",
    "        print(\"Gold Tags:\", ' '.join(gold_tags))\n",
    "        print(\"Predicted Tags:\", ' '.join(pred_tags))\n",
    "\n",
    "    # Step 7: Error Analysis (at least one case)\n",
    "    if errors:\n",
    "        words, gold_tags, pred_tags = errors[0]  # Take the first error case\n",
    "        print(\"\\nError Analysis Example:\")\n",
    "        print(\"Sentence:\", ' '.join(words))\n",
    "        print(\"Gold Tags:\", ' '.join(gold_tags))\n",
    "        print(\"Predicted Tags:\", ' '.join(pred_tags))\n",
    "        print(\"Analysis:\")\n",
    "        for j in range(len(words)):\n",
    "            if pred_tags[j] != gold_tags[j]:\n",
    "                print(f\" - Word '{words[j]}' mis-tagged as {pred_tags[j]} (gold: {gold_tags[j]})\")\n",
    "                if words[j] not in unique_words:\n",
    "                    print(\"   Reason: Unseen word in training data.\")\n",
    "                else:\n",
    "                    print(\"   Possible Reason: Word ambiguity or contextual influence from transitions.\")\n",
    "    else:\n",
    "        print(\"\\nNo errors found in test set for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e2c15-5cf9-4fd2-a44d-3a81fce067fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HMM POS Tagger (venv)",
   "language": "python",
   "name": "hmm_pos_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
